# pages/3_ğŸ”®_SuÃ§_Tahmini_ve_Forecast.py
# --- AmaÃ§ ---
# Bu sayfa, birden fazla/tÃ¼m suÃ§ kategorisi ve GEOID seÃ§imiyle geÃ§miÅŸ 5 yÄ±llÄ±k veriden
# saatlik pattern Ã§Ä±karÄ±r, basit fakat aÃ§Ä±klanabilir bir forecast Ã¼retir ve
# harita Ã¼zerinde (hover) her hÃ¼cre iÃ§in en olasÄ± 3 suÃ§ tÃ¼rÃ¼ ve en olasÄ± zaman aralÄ±klarÄ±nÄ± gÃ¶sterir.
# Veri kaynaklarÄ± GitHub Actions artifact'larÄ±ndan okunur:
#  - fr-crime-pipeline-output: fr_crime_09.parquet  (geÃ§miÅŸ olay+Ã¶zellikler)
#  - crime_prediction_data / sf-crime-parquet: risk_hourly.parquet (varsa genel risk/ÅŸimdiki skor)
#
# Not: Bu forecast, kolluk iÃ§in yalÄ±n ve hÄ±zlÄ± Ã§alÄ±ÅŸacak ÅŸekilde tasarlandÄ±:
#  - â€œSon X gÃ¼n + saat-of-day + gÃ¼n-of-weekâ€ aÄŸÄ±rlÄ±klÄ± bir profil hesabÄ± (recency-weighting)
#  - AynÄ± saat dilimi paternine gÃ¶re Ã¶nÃ¼mÃ¼zdeki 24/48/168 saat iÃ§in beklenen olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ±
#  - Top 3 kategori ve bu kategorilerin %95 kitlesini taÅŸÄ±yan pik saat aralÄ±klarÄ±
#  - Harita hover tooltip: Top-3 + tavsiye saat bloklarÄ±
#
# GeliÅŸtirme noktalarÄ± (ileride):
#  - Mevcut stacking model skorlarÄ±nÄ± kategori-bazlÄ± hale getirmek
#  - 911/311 kÄ±sa vadeli sinyalleriyle nowcast dÃ¼zeltmesi
#  - ETS/Prophet/TFT gibi modellerle GEOIDÃ—kategori bazlÄ± ileri forecast

import io, os, json, zipfile
from datetime import datetime, timedelta, date
from typing import List, Dict, Tuple
import pandas as pd
import numpy as np
import streamlit as st
import pydeck as pdk
import requests
import altair as alt

# Yerel bileÅŸenler
from components.last_update import show_last_update_badge
from components.meta import MODEL_VERSION, MODEL_LAST_TRAIN

# ---------- Sayfa baÅŸlÄ±k & ayarlar ----------
st.set_page_config(page_title="ğŸ”® SuÃ§ Tahmini ve Forecast", layout="wide")
st.title("ğŸ”® SuÃ§ Tahmini â€¢ Kategori & GEOID SeÃ§imli")
st.caption("5 yÄ±llÄ±k tarihsel desenlere dayalÄ± hÄ±zlÄ± forecast ve gÃ¶rsel Ã¶zetler")

# ---------- Konfig ----------
cfg = st.secrets if hasattr(st, "secrets") else {}
# Artifacts
OWNER_MAIN = cfg.get("artifact_owner", "cem5113")
REPO_MAIN = cfg.get("artifact_repo", "crime_prediction_data")
ARTIFACT_RISK = cfg.get("artifact_name", "sf-crime-parquet")
EXPECTED_RISK_PARQUET = "risk_hourly.parquet"  # opsiyonel

OWNER_FR = cfg.get("fr_owner", OWNER_MAIN)
REPO_FR = cfg.get("fr_repo", "fr-crime-pipeline-output")
ARTIFACT_FR = cfg.get("fr_artifact", "fr-crime-pipeline-output")
EXPECTED_FR_PARQUET = cfg.get("fr_file", "fr_crime_09.parquet")

# GeoJSON
GEOJSON_PATH_LOCAL_DEFAULT = cfg.get("geojson_path", "data/sf_cells.geojson")
RAW_GEOJSON_OWNER = cfg.get("geojson_owner", "cem5113")
RAW_GEOJSON_REPO = cfg.get("geojson_repo", "crimepredict")

# ---------- YardÄ±mcÄ±lar: Token & GitHub ----------
def resolve_github_token() -> str | None:
    tok = os.getenv("GITHUB_TOKEN")
    if tok:
        return tok
    for k in ("github_token", "GH_TOKEN", "GITHUB_TOKEN"):
        try:
            if k in st.secrets and st.secrets[k]:
                os.environ["GITHUB_TOKEN"] = str(st.secrets[k])
                return os.environ["GITHUB_TOKEN"]
        except Exception:
            pass
    return None

def gh_headers() -> dict:
    hdrs = {"Accept": "application/vnd.github+json"}
    tok = os.getenv("GITHUB_TOKEN")
    if tok:
        hdrs["Authorization"] = f"Bearer {tok}"
    return hdrs

@st.cache_data(show_spinner=True, ttl=15*60)
def fetch_latest_artifact_zip(owner: str, repo: str) -> bytes:
    base = f"https://api.github.com/repos/{owner}/{repo}/actions/artifacts"
    r = requests.get(base, headers=gh_headers(), timeout=30)
    r.raise_for_status()
    items = r.json().get("artifacts", [])
    # En gÃ¼ncel ve sÃ¼resi geÃ§memiÅŸ olanÄ± seÃ§
    cand = [a for a in items if not a.get("expired", False)]
    if not cand:
        raise FileNotFoundError("Uygun artifact bulunamadÄ±.")
    cand.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
    url = cand[0].get("archive_download_url")
    if not url:
        raise RuntimeError("archive_download_url yok")
    r2 = requests.get(url, headers=gh_headers(), timeout=60)
    r2.raise_for_status()
    return r2.content

@st.cache_data(show_spinner=True, ttl=15*60)
def read_parquet_from_artifact(owner: str, repo: str, wanted_suffix: str) -> pd.DataFrame:
    zip_bytes = fetch_latest_artifact_zip(owner, repo)
    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:
        memlist = zf.namelist()
        matches = [n for n in memlist if n.endswith("/" + wanted_suffix) or n.endswith(wanted_suffix)]
        if not matches:
            raise FileNotFoundError(f"Zip iÃ§inde {wanted_suffix} yok. Ã–rnek iÃ§erik: {memlist[:15]}")
        with zf.open(matches[0]) as f:
            df = pd.read_parquet(f)
    # Kolon isimlerini normalize et
    df.columns = [str(c).strip() for c in df.columns]
    return df

# ---------- GeoJSON getir ----------
@st.cache_data(show_spinner=True, ttl=60*60)
def fetch_geojson_smart(path_local: str, path_in_zip: str, raw_owner: str, raw_repo: str) -> dict:
    # 1) Local
    try:
        if os.path.exists(path_local):
            with open(path_local, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    # 2) risk artifact iÃ§inde olabilir
    try:
        zip_bytes = fetch_latest_artifact_zip(OWNER_MAIN, REPO_MAIN)
        with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:
            memlist = zf.namelist()
            candidates = [n for n in memlist if n.endswith("/" + path_in_zip) or n.endswith(path_in_zip)]
            if candidates:
                with zf.open(candidates[0]) as f:
                    return json.load(io.TextIOWrapper(f, encoding="utf-8"))
    except Exception:
        pass
    # 3) Raw GitHub (public)
    try:
        raw = f"https://raw.githubusercontent.com/{raw_owner}/{raw_repo}/main/{path_local}"
        r = requests.get(raw, timeout=30)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return {}

# ---------- Veri hazÄ±rlÄ±k ----------
def ensure_datetime(df: pd.DataFrame) -> pd.DataFrame:
    if "datetime" in df.columns:
        df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")
    elif "date" in df.columns and "event_hour" in df.columns:
        df["datetime"] = pd.to_datetime(df["date"].astype(str) + " " + df["event_hour"].astype(str) + ":00:00", errors="coerce")
    else:
        # En azÄ±ndan date alanÄ± olsun
        if "date" in df.columns:
            df["datetime"] = pd.to_datetime(df["date"], errors="coerce")
        else:
            # son Ã§are: index
            df["datetime"] = pd.to_datetime(df.index, errors="coerce")
    df["date"] = df["datetime"].dt.date
    df["hour"] = df["datetime"].dt.hour
    df["dow"] = df["datetime"].dt.dayofweek  # 0=Mon
    return df

def normalize_geoid(x) -> str:
    s = ''.join(ch for ch in str(x) if str(ch).isdigit())
    return s.zfill(11)[:11]

# Recency aÄŸÄ±rlÄ±ÄŸÄ± (Ã¶r: yarÄ±-Ã¶mÃ¼r 14 gÃ¼n)
def recency_weight(ts: pd.Series, halflife_days: float = 14.0) -> pd.Series:
    if ts.empty:
        return ts
    max_t = ts.max()
    dt_days = (max_t - ts).dt.total_seconds()/86400.0
    lam = np.log(2.0)/max(halflife_days, 1e-6)
    return np.exp(-lam * dt_days)

# GEOIDÃ—kategoriÃ—saat bazlÄ± profile Ã§Ä±kar
@st.cache_data(show_spinner=True, ttl=10*60)
def build_hourly_profile(df: pd.DataFrame, last_n_days: int = 90, halflife_days: float = 14.0,
                         use_y_label: bool = True) -> pd.DataFrame:
    base = df.copy()
    base = base.sort_values("datetime")
    # Son N gÃ¼n filtresi
    if last_n_days:
        cutoff = base["datetime"].max() - pd.Timedelta(days=last_n_days)
        base = base.loc[base["datetime"] >= cutoff]
    # AÄŸÄ±rlÄ±k: zaman yakÄ±nlÄ±ÄŸÄ±
    w = recency_weight(base["datetime"], halflife_days=halflife_days)
    base["w"] = w.values
    # Olay sinyali: Y_label varsa onu kullan, yoksa 1
    if use_y_label and "Y_label" in base.columns:
        sig = (base["Y_label"].astype(float).clip(0,1))
    else:
        sig = 1.0
    base["signal"] = sig

    # GEOID normalizasyonu
    if "GEOID" in base.columns:
        base["geoid"] = base["GEOID"].map(normalize_geoid)
    elif "geoid" in base.columns:
        base["geoid"] = base["geoid"].map(normalize_geoid)
    elif "id" in base.columns:
        base["geoid"] = base["id"].map(normalize_geoid)
    else:
        base["geoid"] = ""

    # Eksik kategori -> "Unknown"
    if "category" not in base.columns:
        base["category"] = "Unknown"

    # GÃ¼n/ saat
    base = ensure_datetime(base)

    # GÃ¼n-of-week conditioning (dilersek benzer gÃ¼nleri aÄŸÄ±r basacak ÅŸekilde)
    base["dow"] = base["dow"].fillna(-1).astype(int)

    # Weighted olay beklentisi: geoidÃ—categoryÃ—hour (ve opsiyonel DOW)
    grp = base.groupby(["geoid", "category", "dow", "hour"], as_index=False).agg(
        exp_events=("signal", lambda x: float(np.sum(x))) ,
        w_mean=("w", "mean"),
        n=("signal", "size")
    )
    # beklenen skor = exp_events * w_mean  (basitÃ§e aÄŸÄ±rlÄ±klandÄ±rma)
    grp["score"] = grp["exp_events"] * grp["w_mean"]
    # Normalizasyon: her geoidÃ—dow iÃ§in kategoriÃ—saat skoru -> olasÄ±lÄ±ÄŸa Ã¶lÃ§ekle
    norm = grp.groupby(["geoid", "dow"], as_index=False)["score"].sum().rename(columns={"score":"tot"})
    out = grp.merge(norm, on=["geoid","dow"], how="left")
    out["prob"] = np.where(out["tot"]>0, out["score"]/out["tot"], 0.0)
    return out  # kolonlar: geoid, category, dow, hour, prob

# SeÃ§ili DOW senaryosuna gÃ¶re (Ã¶r. forecast baÅŸlangÄ±Ã§ gÃ¼nÃ¼nÃ¼n DOW'u) next H saat iÃ§in daÄŸÄ±tÄ±m
@st.cache_data(show_spinner=True, ttl=10*60)
def forecast_next_hours(profile: pd.DataFrame, start_dt: datetime, horizon_h: int = 24) -> pd.DataFrame:
    if profile.empty:
        return profile
    rows = []
    for h in range(horizon_h):
        t = start_dt + timedelta(hours=h)
        dow = t.weekday()
        hr = t.hour
        # geoidÃ—categoryÃ—dowÃ—hour prob
        sl = profile[(profile["dow"]==dow) & (profile["hour"]==hr)]
        if sl.empty:
            # fallback: dow baÄŸÄ±msÄ±z
            sl = profile[profile["hour"]==hr].copy()
        if sl.empty:
            continue
        tmp = sl.copy()
        tmp["ts"] = t
        rows.append(tmp)
    fc = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=list(profile.columns)+["ts"]) 
    return fc  # kolonlar: geoid, category, dow, hour, prob, ts

# Top-3 kategori ve saat bloklarÄ±nÄ± biÃ§imle

def contiguous_blocks(hours: List[int]) -> List[Tuple[int,int]]:
    if not hours: return []
    hours = sorted(set(int(h)%24 for h in hours))
    blocks = []
    start = prev = hours[0]
    for h in hours[1:]+[hours[0]]:  # wrap check
        if (h-prev) % 24 != 1:
            blocks.append((start, prev))
            start = h
        prev = h
    if blocks and blocks[0] == (start, prev):
        return [blocks[0]]
    blocks.append((start, prev))
    # En fazla ilk 2 blok
    return blocks[:2]

def format_block(b: Tuple[int,int]) -> str:
    a,bh = b
    if a==bh:
        return f"{a:02d}:00"
    # sarma varsa iki parÃ§a gÃ¶ster
    if (bh - a) % 24 < (a - bh) % 24:
        return f"{a:02d}:00â€“{bh:02d}:59"
    return f"{a:02d}:00â€“{bh:02d}:59"

@st.cache_data(show_spinner=True, ttl=10*60)
def summarize_top3(fc: pd.DataFrame, geoid_list: List[str]) -> pd.DataFrame:
    if fc.empty:
        return pd.DataFrame(columns=["geoid","rank","category","share","peak_hours"])
    sl = fc[fc["geoid"].isin(geoid_list)].copy()
    # geoidÃ—category toplam olasÄ±lÄ±k payÄ± (horizon iÃ§inde)
    g = sl.groupby(["geoid","category"], as_index=False)["prob"].sum().rename(columns={"prob":"prob_sum"})
    # geoid bazÄ±nda normalize
    tot = g.groupby("geoid", as_index=False)["prob_sum"].sum().rename(columns={"prob_sum":"tot"})
    g = g.merge(tot, on="geoid", how="left")
    g["share"] = np.where(g["tot"]>0, g["prob_sum"]/g["tot"], 0.0)
    # Top-3 seÃ§
    ranks = []
    for geoid, sub in g.groupby("geoid"):
        sub = sub.sort_values("share", ascending=False).head(3)
        # her kategori iÃ§in pik saatleri bul: prob'a gÃ¶re ilk %30 kitleyi taÅŸÄ±yan saatler
        peaks = []
        for _, r in sub.iterrows():
            cat = r["category"]
            hh = (
                sl[(sl["geoid"]==geoid) & (sl["category"]==cat)]
                .groupby("hour", as_index=False)["prob"].mean()
                .sort_values("prob", ascending=False)
            )
            if not hh.empty:
                csum = (hh["prob"].cumsum()/hh["prob"].sum()).fillna(0)
                top_hours = hh.loc[csum<=0.30, "hour"].astype(int).tolist()
                # saat bloklarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r (en fazla 2 blok)
                blks = contiguous_blocks(top_hours) if top_hours else []
                label = ", ".join(format_block(b) for b in blks) if blks else "â€”"
            else:
                label = "â€”"
            peaks.append(label)
        sub["peak_hours"] = peaks
        sub["rank"] = range(1, len(sub)+1)
        ranks.append(sub[["geoid","rank","category","share","peak_hours"]])
    return pd.concat(ranks, ignore_index=True) if ranks else pd.DataFrame(columns=["geoid","rank","category","share","peak_hours"])

# ---------- Harita zenginleÅŸtirme ----------
COLOR_MAP = {
    "Ã§ok dÃ¼ÅŸÃ¼k": [200,200,200],
    "dÃ¼ÅŸÃ¼k": [56,168,0],
    "orta": [255,221,0],
    "yÃ¼ksek": [204,0,0],
}

def only_digits(s: str) -> str:
    return ''.join(ch for ch in str(s) if ch.isdigit())

def inject_properties_top3(geojson_dict: dict, top3_df: pd.DataFrame, risk_daily_df: pd.DataFrame | None = None) -> dict:
    if not geojson_dict:
        return geojson_dict
    feats = geojson_dict.get("features", [])
    # quick lookup
    tmap: Dict[str, pd.DataFrame] = {
        g: sub.sort_values("rank").reset_index(drop=True)
        for g, sub in top3_df.groupby("geoid")
    }
    rmap = None
    if risk_daily_df is not None and not risk_daily_df.empty:
        rmap = risk_daily_df.set_index("geoid")["risk_score_daily"].to_dict()
        # eÅŸiklere gÃ¶re renk
        if not risk_daily_df.empty:
            q25, q50, q75 = (
                risk_daily_df["risk_score_daily"].quantile([0.25,0.5,0.75]).tolist()
            )
    out = []
    for feat in feats:
        props = (feat.get("properties") or {}).copy()
        raw = next((props[k] for k in ("geoid","GEOID","cell_id","id") if k in props), None)
        geoid = only_digits(raw)[:11] if raw is not None else ""
        props.setdefault("display_id", str(raw or ""))

        # risk renkleri (varsa)
        lvl = "orta"
        if rmap is not None and geoid in rmap:
            val = float(rmap[geoid])
            props["risk_score_daily"] = val
            # basit renkleme: q25/50/75
            if val <= q25: lvl = "dÃ¼ÅŸÃ¼k"
            elif val <= q50: lvl = "orta"
            elif val <= q75: lvl = "yÃ¼ksek"
            else: lvl = "yÃ¼ksek"
        props["fill_color"] = COLOR_MAP.get(lvl, [220,220,220])

        # Top-3 kategori + saat bloklarÄ±
        if geoid in tmap:
            sub = tmap[geoid]
            lines = []
            for i, rr in sub.iterrows():
                pct = f"{rr['share']*100:.1f}%"
                ph = rr["peak_hours"]
                lines.append(f"{rr['rank']}. {rr['category']} â€¢ {pct} â€¢ {ph}")
            props["top3_html"] = "<br/>".join(lines)
        else:
            props["top3_html"] = "Veri yok"

        out.append({**feat, "properties": props})
    return {**geojson_dict, "features": out}

# ---------- UI: Yan panel ----------
TOKEN = resolve_github_token()
st.sidebar.header("Veri KaynaklarÄ±")
with st.sidebar.expander("ğŸ” Token Durumu", expanded=TOKEN is None):
    st.write("Env GITHUB_TOKEN:", "âœ…" if os.getenv("GITHUB_TOKEN") else "âŒ")
    has_secret = False
    try:
        has_secret = any(k in st.secrets for k in ("github_token","GH_TOKEN","GITHUB_TOKEN"))
    except Exception:
        pass
    st.write("Secrets'ta Token:", "âœ…" if has_secret else "âŒ")

refresh = st.sidebar.button("Veriyi Yenile (artifact'larÄ± tazele)")
if refresh:
    fetch_latest_artifact_zip.clear()
    read_parquet_from_artifact.clear()
    fetch_geojson_smart.clear()
    build_hourly_profile.clear()
    forecast_next_hours.clear()
    summarize_top3.clear()

if not TOKEN:
    st.error("GitHub token yok. `st.secrets['github_token']` veya GITHUB_TOKEN env ayarlayÄ±n.")
    st.stop()

# ---------- Veri yÃ¼kleme ----------
try:
    df_events = read_parquet_from_artifact(OWNER_FR, REPO_FR, EXPECTED_FR_PARQUET)
except Exception as e:
    st.error(f"Olay verisi (fr_crime_09.parquet) okunamadÄ±: {e}")
    st.stop()

# Opsiyonel risk dosyasÄ± (genel renk iÃ§in)
try:
    df_risk = read_parquet_from_artifact(OWNER_MAIN, REPO_MAIN, EXPECTED_RISK_PARQUET)
    # gÃ¼nlÃ¼k ortalama
    if not df_risk.empty:
        cols = [c.lower().strip() for c in df_risk.columns]
        df_risk.columns = cols
        # risk_score/ geoid / date esnek eÅŸleme
        if "risk_score" not in df_risk.columns:
            for alt in ("risk","score","prob","probability"):
                if alt in df_risk.columns:
                    df_risk = df_risk.rename(columns={alt:"risk_score"}); break
        if "geoid" not in df_risk.columns:
            for alt in ("cell_id","geoid10","geoid_10","id"):
                if alt in df_risk.columns:
                    df_risk["geoid"] = df_risk[alt]; break
        if "date" in df_risk.columns:
            df_risk["date"] = pd.to_datetime(df_risk["date"]).dt.date
            risk_daily = (
                df_risk.groupby([df_risk["geoid"].map(lambda x: ''.join(ch for ch in str(x) if str(ch).isdigit()).zfill(11)[:11]), "date"], as_index=False)["risk_score"].mean()
                .rename(columns={"geoid":"geoid_norm","risk_score":"risk_score_daily"})
            )
            # Son gÃ¼n
            if not risk_daily.empty:
                last_day = max(risk_daily["date"])
                risk_today = risk_daily[risk_daily["date"]==last_day].copy()
                risk_today = risk_today.rename(columns={"geoid_norm":"geoid"})
            else:
                risk_today = pd.DataFrame(columns=["geoid","risk_score_daily"]) 
        else:
            risk_today = pd.DataFrame(columns=["geoid","risk_score_daily"]) 
    else:
        risk_today = pd.DataFrame(columns=["geoid","risk_score_daily"]) 
except Exception:
    risk_today = pd.DataFrame(columns=["geoid","risk_score_daily"]) 

# ---------- Veri temizleme ----------
df_events = ensure_datetime(df_events)
# GEOID
if "geoid" in df_events.columns:
    df_events["geoid"] = df_events["geoid"].map(normalize_geoid)
elif "GEOID" in df_events.columns:
    df_events["geoid"] = df_events["GEOID"].map(normalize_geoid)
elif "id" in df_events.columns:
    df_events["geoid"] = df_events["id"].map(normalize_geoid)
else:
    df_events["geoid"] = ""

# Kategoriler
if "category" not in df_events.columns:
    df_events["category"] = "Unknown"

# KullanÄ±cÄ±ya seÃ§imler iÃ§in listeler
all_categories = sorted([str(x) for x in df_events["category"].dropna().unique().tolist()])
all_geoids = sorted(df_events["geoid"].dropna().unique().tolist())

# ---------- Sidebar Filtreleri ----------
st.sidebar.header("Filtreler")
sel_cats = st.sidebar.multiselect("SuÃ§ kategorileri", options=all_categories, default=[])
sel_geoids = st.sidebar.multiselect("GEOID seÃ§imi", options=all_geoids[:5000], default=[], help="Arama kutusunu kullanarak GEOID filtreleyin")

min_dt = df_events["datetime"].min()
max_dt = df_events["datetime"].max()
col1, col2 = st.sidebar.columns(2)
with col1:
    start_date = st.date_input("BaÅŸlangÄ±Ã§ tarihi (profil iÃ§in)", value=(max_dt.date() - timedelta(days=90)), min_value=min_dt.date(), max_value=max_dt.date())
with col2:
    halflife = st.number_input("YarÄ±-Ã¶mÃ¼r (gÃ¼n)", min_value=3.0, max_value=60.0, value=14.0, step=1.0, help="Recency aÄŸÄ±rlÄ±ÄŸÄ±")

horizon = st.sidebar.select_slider("Horizon (saat)", options=[24, 48, 72, 168], value=24)
fc_start = st.sidebar.datetime_input("Forecast baÅŸlangÄ±cÄ±", value=datetime.combine(max_dt.date(), datetime.min.time()), help="VarsayÄ±lan: veri son gÃ¼nÃ¼nÃ¼n 00:00")

# Alt filtreler
if sel_cats:
    df_events = df_events[df_events["category"].isin(sel_cats)]
if sel_geoids:
    df_events = df_events[df_events["geoid"].isin(sel_geoids)]

# Profili Ã¼ret
last_n_days = (max_dt.date() - start_date).days if start_date else 90
profile = build_hourly_profile(df_events, last_n_days=last_n_days, halflife_days=float(halflife), use_y_label=True)

if profile.empty:
    st.warning("Profil Ã¼retilemedi. Filtreleri gevÅŸetin veya veri kaynaÄŸÄ±nÄ± kontrol edin.")
    st.stop()

# Forecast
fc = forecast_next_hours(profile, start_dt=fc_start, horizon_h=int(horizon))

# Top-3 Ã¶zet
geoids_in_scope = sel_geoids if sel_geoids else all_geoids
summary_top3 = summarize_top3(fc, geoid_list=geoids_in_scope)

# ---------- Harita ----------
col_map, col_tbl = st.columns([2,1])
with col_map:
    gj = fetch_geojson_smart(GEOJSON_PATH_LOCAL_DEFAULT, GEOJSON_PATH_LOCAL_DEFAULT, RAW_GEOJSON_OWNER, RAW_GEOJSON_REPO)
    enriched = inject_properties_top3(gj, summary_top3, risk_today if not risk_today.empty else None)
    layer = pdk.Layer(
        "GeoJsonLayer",
        enriched,
        stroked=True,
        get_line_color=[80,80,80],
        line_width_min_pixels=0.5,
        filled=True,
        get_fill_color="properties.fill_color",
        pickable=True,
        opacity=0.65,
    )
    tooltip = {
        "html": (
            "<b>GEOID:</b> {display_id}<br/>"
            "<b>Top-3 (pay â€¢ saat bloklarÄ±):</b><br/>{top3_html}"
        ),
        "style": {"backgroundColor":"#262730", "color":"white"},
    }
    deck = pdk.Deck(
        layers=[layer],
        initial_view_state=pdk.ViewState(latitude=37.7749, longitude=-122.4194, zoom=10),
        map_style="light",
        tooltip=tooltip,
    )
    st.pydeck_chart(deck, use_container_width=True)

with col_tbl:
    st.subheader("SeÃ§im Ã–zeti â€“ Top 3")
    if not summary_top3.empty:
        show = summary_top3.copy()
        show["share"] = (show["share"]*100).map(lambda x: f"{x:.1f}%")
        st.dataframe(show.rename(columns={"rank":"SÄ±ra","category":"Kategori","share":"Pay","peak_hours":"Pik Saatler"}), use_container_width=True)
    else:
        st.info("Top-3 Ã¶zeti iÃ§in veri yok.")

# ---------- Detay Grafikler ----------
st.markdown("---")
st.subheader("Detay: GEOID Ã— Kategori Ã— Saat Profili / Forecast")

# Grafik iÃ§in seÃ§im
sel_geo_for_plot = st.multiselect("Grafik iÃ§in GEOID seÃ§in", options=geoids_in_scope[:1000], default=geoids_in_scope[:1])

if sel_geo_for_plot:
    # Saat bazlÄ± Ä±sÄ± haritasÄ± (kategoriÃ—saat)
    plot_df = (
        fc[fc["geoid"].isin(sel_geo_for_plot)]
        .groupby(["category","hour"], as_index=False)["prob"].mean()
    )
    if not plot_df.empty:
        heat = alt.Chart(plot_df).mark_rect().encode(
            x=alt.X("hour:O", title="Saat"),
            y=alt.Y("category:N", title="Kategori"),
            tooltip=[alt.Tooltip("category", title="Kategori"), alt.Tooltip("hour", title="Saat"), alt.Tooltip("prob", title="OlasÄ±lÄ±k", format=".3f")],
            color=alt.Color("prob:Q", title="OlasÄ±lÄ±k", scale=alt.Scale(scheme="orangered"))
        ).properties(height=320)
        st.altair_chart(heat, use_container_width=True)

    # Top kategoriler iÃ§in saatlik Ã§izgi
    topk = (
        summary_top3[summary_top3["geoid"].isin(sel_geo_for_plot)]
        .sort_values(["geoid","rank"]) 
        .groupby("geoid").head(3)[["geoid","category"]].drop_duplicates()
    )
    if not topk.empty:
        lines = (
            fc.merge(topk, on=["geoid","category"], how="inner")
              .groupby(["category","hour"], as_index=False)["prob"].mean()
        )
        line = alt.Chart(lines).mark_line(point=True).encode(
            x=alt.X("hour:O", title="Saat"),
            y=alt.Y("prob:Q", title="OlasÄ±lÄ±k"),
            color=alt.Color("category:N", title="Kategori"),
            tooltip=["category","hour","prob"]
        ).properties(height=320)
        st.altair_chart(line, use_container_width=True)
else:
    st.info("Grafik iÃ§in en az bir GEOID seÃ§in.")

# ---------- AÃ§Ä±klamalar ----------
with st.expander("â„¹ï¸ Metodoloji"):
    st.markdown(
        """
        **Ã–zet**: Forecast, *son N gÃ¼n* verisi Ã¼zerinden saat-of-day ve gÃ¼n-of-week paternlerini **zaman yakÄ±nlÄ±ÄŸÄ±na gÃ¶re aÄŸÄ±rlÄ±klandÄ±rarak** Ã§Ä±karÄ±r. 
        SeÃ§ilen horizon (24/48/72/168 saat) boyunca her GEOIDÃ—kategoriÃ—saat iÃ§in beklenen olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± hesaplanÄ±r. 
        Hover'da gÃ¶sterilen *Top-3* listesi, bu horizon iÃ§indeki toplam olasÄ±lÄ±k payÄ±na gÃ¶re belirlenir ve her kategori iÃ§in olasÄ±lÄ±ÄŸÄ±n %30'unu taÅŸÄ±yan **pik saat bloklarÄ±** verilir.

        **Parametreler**:
        - *BaÅŸlangÄ±Ã§ tarihi*: profile dahil edilecek son gÃ¼n sayÄ±sÄ±nÄ± belirler (Ã¶r. 90 gÃ¼n).
        - *YarÄ±-Ã¶mÃ¼r (gÃ¼n)*: recency aÄŸÄ±rlÄ±ÄŸÄ±nda yarÄ±-Ã¶mÃ¼r. Daha kÃ¼Ã§Ã¼k deÄŸer, son gÃ¼nleri daha baskÄ±n kÄ±lar.
        - *Horizon*: ileriye dÃ¶nÃ¼k saat sayÄ±sÄ± (24/48/72/168).

        **SÄ±nÄ±rlamalar / GeliÅŸtirme Fikirleri**:
        - Kategori-bazlÄ± model skorlarÄ±yla nowcast dÃ¼zeltmesi (911/311 kÄ±sa vadeli sinyaller).
        - Mevsimsellik + tatil etkisi iÃ§in ayrÄ± profiller.
        - Ä°leri seviye zaman serisi modelleri (ETS, Prophet, TFT) ile GEOIDÃ—kategori forecast.
        """
    )

show_last_update_badge(data_upto=None, model_version=MODEL_VERSION, last_train=MODEL_LAST_TRAIN)
