# pages/3_ğŸ§­_SuÃ§_Tahmini.py
# -----------------------------------------------------------
# Basit: Artifact ZIP -> Parquet oku -> SeÃ§ime gÃ¶re anlÄ±k tahmin
# -----------------------------------------------------------

import io, zipfile, requests, math
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import streamlit as st

# ML
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

st.set_page_config(page_title="ğŸ§­ SuÃ§ Tahmini (Demo)", layout="wide")
st.title("ğŸ§­ SuÃ§ Tahmini (Demo)")
st.caption("GitHub artifact'tan veri okunur, kullanÄ±cÄ± seÃ§imine gÃ¶re anlÄ±k tahmin yapÄ±lÄ±r.")

# --- KullanÄ±cÄ± ayarlarÄ± / sabitler ---
ZIP_URL_DEFAULT = "https://github.com/cem5113/crime_prediction_data/releases/latest/download/fr-crime-outputs-parquet.zip"
MAX_ROWS_PREVIEW = 1000

# -----------------------------------------------------------
# YardÄ±mcÄ±lar
# -----------------------------------------------------------
def _find_col(df, candidates, default=None):
    """Aday isimler iÃ§inden ilk mevcut olan sÃ¼tunu dÃ¶ndÃ¼r."""
    for c in candidates:
        if c in df.columns:
            return c
    return default

def _to_datetime_safe(s, fallback_format=None):
    try:
        return pd.to_datetime(s, errors="coerce")
    except Exception:
        if fallback_format:
            return pd.to_datetime(s, format=fallback_format, errors="coerce")
        return pd.to_datetime(s, errors="coerce")

@st.cache_data(show_spinner=True, ttl=60*30)  # 30 dk cache
def load_from_artifact(zip_url: str):
    """Artifact ZIP indir, gerekli parquet'leri oku."""
    resp = requests.get(zip_url, timeout=60)
    resp.raise_for_status()
    zf = zipfile.ZipFile(io.BytesIO(resp.content))
    names = zf.namelist()

    # Dosya yollarÄ± esnek olsun
    def pick(name_part):
        for n in names:
            if name_part in n and n.endswith(".parquet"):
                return n
        return None

    data_fp = pick("fr_crime_09") or pick("fr_crime_10") or pick("fr_crime")  # en zengin ilk
    metrics_fp = pick("metrics_stacking_ohe")

    if not data_fp:
        raise FileNotFoundError("ZIP iÃ§inde fr_crime_09/10/parquet bulunamadÄ±.")

    df = pd.read_parquet(zf.open(data_fp))
    metrics = None
    if metrics_fp:
        try:
            metrics = pd.read_parquet(zf.open(metrics_fp))
        except Exception:
            metrics = None
    return df, metrics, names

def pick_last_12m(df, date_col):
    """Son 12 aya indirger (varsa)."""
    if date_col is None:
        return df
    dts = _to_datetime_safe(df[date_col])
    max_dt = dts.max()
    if pd.isna(max_dt):
        return df
    start_dt = max_dt - pd.Timedelta(days=365)
    return df.loc[(dts >= start_dt) & (dts <= max_dt)].copy()

def build_model(train_df, target_col="Y_label", exclude_cols=None, seed=42):
    """HÄ±zlÄ± bir XGBClassifier kur."""
    if exclude_cols is None:
        exclude_cols = []
    cols_num = train_df.select_dtypes(include=["number", "bool"]).columns.tolist()
    cols_num = [c for c in cols_num if c != target_col and c not in exclude_cols]

    X = train_df[cols_num]
    y = train_df[target_col].astype(int)

    # Basit train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.20, random_state=seed, stratify=y if y.nunique()==2 else None
    )

    # Hafif model (hÄ±zlÄ±)
    model = XGBClassifier(
        n_estimators=220,
        learning_rate=0.06,
        max_depth=6,
        subsample=0.85,
        colsample_bytree=0.85,
        reg_lambda=1.0,
        n_jobs=0,
        random_state=seed,
        tree_method="hist"
    )
    model.fit(X_train, y_train)
    y_pred = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_pred) if len(np.unique(y_test))==2 else np.nan

    return model, cols_num, float(auc)

@st.cache_resource(show_spinner=True)
def train_cached(train_df, target_col, exclude_cols):
    return build_model(train_df, target_col=target_col, exclude_cols=exclude_cols)

# -----------------------------------------------------------
# 1) Artifact'tan veri yÃ¼kle
# -----------------------------------------------------------
with st.sidebar:
    st.subheader("âš™ï¸ Veri KaynaÄŸÄ±")
    zip_url = st.text_input("Artifact ZIP URL", ZIP_URL_DEFAULT)
    if st.button("ğŸ”„ Veriyi Ä°ndir & YÃ¼kle"):
        st.session_state["_reload"] = True

# Ä°lk yÃ¼kleme
if "_reload" not in st.session_state:
    st.session_state["_reload"] = True

if st.session_state["_reload"]:
    try:
        df_raw, metrics_df, zip_names = load_from_artifact(zip_url)
        st.session_state["df_raw"] = df_raw
        st.session_state["metrics_df"] = metrics_df
        st.session_state["zip_names"] = zip_names
        st.session_state["_reload"] = False
    except Exception as e:
        st.error(f"Veri indirilemedi: {e}")
        st.stop()

df_raw = st.session_state["df_raw"].copy()
metrics_df = st.session_state.get("metrics_df")
zip_names = st.session_state.get("zip_names", [])

with st.expander("ZIP iÃ§eriÄŸi"):
    st.code("\n".join(zip_names) or "(boÅŸ)")

st.success(f"âœ… Veri yÃ¼klendi: {df_raw.shape[0]:,} satÄ±r Ã— {df_raw.shape[1]:,} sÃ¼tun")

# -----------------------------------------------------------
# 2) SÃ¼tun tespiti ve son 12 ay filtresi
# -----------------------------------------------------------
# Temel kolonlar
geoid_col = _find_col(df_raw, ["GEOID", "geoid", "Key", "key"], default=None)
date_col  = _find_col(df_raw, ["date", "Date", "dt", "event_date"], default=None)
hour_col  = _find_col(df_raw, ["event_hour_x", "event_hour", "hour", "event_hour_y"], default=None)
cat_col   = _find_col(df_raw, ["category", "Category", "cat"], default=None)
y_col     = _find_col(df_raw, ["Y_label", "label", "y"], default="Y_label")

if geoid_col is None or y_col not in df_raw.columns:
    st.error("Gerekli sÃ¼tunlar yok: en azÄ±ndan GEOID ve Y_label bulunmalÄ±.")
    st.stop()

df = df_raw.copy()
df[y_col] = df[y_col].astype(int)

# Son 12 ay (varsa date)
df_12m = pick_last_12m(df, date_col)
if len(df_12m) < len(df):
    st.info(f"â±ï¸ Model eÄŸitimi iÃ§in son 12 ay seÃ§ildi: {len(df_12m):,} satÄ±r.")
else:
    st.info("â±ï¸ Tarih sÃ¼tunu tespit edilemedi veya uygun deÄŸil; tÃ¼m veri kullanÄ±lacak.")
df_train = df_12m

# -----------------------------------------------------------
# 3) Kenar panel: seÃ§imler
# -----------------------------------------------------------
with st.sidebar:
    st.subheader("ğŸ” SeÃ§imler")
    # Kategori seÃ§imi (opsiyonel)
    cats = sorted(df[cat_col].dropna().unique().tolist()) if cat_col else []
    cat_sel = st.selectbox("SuÃ§ kategorisi", ["(tÃ¼mÃ¼)"] + cats, index=0)

    # Saat seÃ§imi
    if hour_col and df[hour_col].notna().any():
        min_h = int(pd.Series(df[hour_col]).min())
        max_h = int(pd.Series(df[hour_col]).max())
        hour_sel = st.slider("Saat (event_hour)", min_value=min(0, min_h), max_value=max(23, max_h), value=12, step=1)
    else:
        hour_sel = st.slider("Saat (kolon bulunamadÄ±, filtre gÃ¶rsel amaÃ§lÄ±)", 0, 23, 12, 1)

    # GEOID seÃ§imi
    geoids = df[geoid_col].dropna().astype(str).unique().tolist()
    default_geoid = geoids[0] if geoids else ""
    geoid_sel = st.text_input("GEOID", value=default_geoid)

    # EÄŸitim tetikleyici
    go_train = st.button("ğŸš€ Modeli EÄŸit ve Tahmin Yap")

# -----------------------------------------------------------
# 4) Veri altkÃ¼meleri ve model
# -----------------------------------------------------------
# GÃ¶sterim filtresi (sadece Ã¶nizleme ve kullanÄ±cÄ± tahmini iÃ§in)
show_df = df.copy()
if cat_col and cat_sel != "(tÃ¼mÃ¼)":
    show_df = show_df.loc[show_df[cat_col] == cat_sel]
if hour_col:
    show_df = show_df.loc[show_df[hour_col] == hour_sel]

st.markdown("### ğŸ“‹ Veri Ã–nizleme")
st.dataframe(show_df.head(MAX_ROWS_PREVIEW))

# EÄŸitim setinden bazÄ± kolonlarÄ± hariÃ§ tut (ID/kimliksel metinler)
exclude_cols = [c for c in ["id", "ID", "datetime", "received_time", geoid_col] if c in df_train.columns]

if go_train:
    with st.spinner("Model eÄŸitiliyor..."):
        model, used_cols, auc = train_cached(df_train, target_col=y_col, exclude_cols=exclude_cols)
    st.success(f"ğŸ¯ ROC-AUC: {auc:.3f}" if not math.isnan(auc) else "ğŸ¯ ROC-AUC hesaplanamadÄ±")

    # KullanÄ±cÄ± kombinasyonu iÃ§in Ã¶rnek satÄ±rlar
    q = df.copy()
    if cat_col and cat_sel != "(tÃ¼mÃ¼)":
        q = q.loc[q[cat_col] == cat_sel]
    if hour_col:
        q = q.loc[q[hour_col] == hour_sel]
    if geoid_sel:
        q = q.loc[q[geoid_col].astype(str) == str(geoid_sel)]

    if len(q) == 0:
        st.warning("Bu seÃ§imlerle satÄ±r bulunamadÄ±. Filteleri geniÅŸletmeyi deneyin.")
    else:
        # Tahmin
        # Not: EÄŸitimde kullandÄ±ÄŸÄ±mÄ±z sayÄ±sal sÃ¼tunlarÄ± alalÄ±m; eksikler olursa dolduralÄ±m.
        Xq = q.reindex(columns=used_cols, fill_value=0)
        try:
            q_pred = model.predict_proba(Xq)[:, 1]
        except Exception:
            # TÃ¼r uyuÅŸmazlÄ±klarÄ± iÃ§in emniyet
            Xq = Xq.apply(pd.to_numeric, errors="coerce").fillna(0)
            q_pred = model.predict_proba(Xq)[:, 1]

        q = q.iloc[:len(q_pred)].copy()
        q["risk_score"] = q_pred

        # GEOID bazÄ±nda Ã¶zet + metrik (varsa)
        geo_mean = float(q["risk_score"].mean())
        st.markdown("### ğŸ”® Tahmin Sonucu")
        cols = st.columns(3)
        cols[0].metric("SeÃ§ime gÃ¶re ortalama risk", f"{geo_mean*100:.1f}%")
        if metrics_df is not None and geoid_col in metrics_df.columns:
            m_sub = metrics_df.loc[metrics_df[geoid_col].astype(str) == str(geoid_sel)]
            # Aday metrik isimleri
            met_name = _find_col(m_sub, ["f1_score","roc_auc","auc","accuracy"], default=None)
            if met_name and len(m_sub):
                mval = float(pd.to_numeric(m_sub[met_name], errors="coerce").dropna().mean())
                cols[1].metric(f"GEOID baÅŸarÄ± ({met_name})", f"{mval:.3f}")
        cols[2].metric("SatÄ±r sayÄ±sÄ± (tahmin)", f"{len(q):,}")

        with st.expander("ğŸ” DetaylÄ± Tahmin SatÄ±rlarÄ±"):
            st.dataframe(q[[geoid_col] + ([cat_col] if cat_col else []) + ([hour_col] if hour_col else []) + ["risk_score"]].head(1000))

        # Top-N riskli GEOID (seÃ§ime gÃ¶re)
        topk = (q.groupby(geoid_col)["risk_score"].mean().sort_values(ascending=False).head(10))
        st.markdown("### ğŸš¨ En Riskli 10 GEOID (mevcut filtrelerle)")
        st.dataframe(topk.reset_index(names=[geoid_col]).rename(columns={"risk_score":"avg_risk"}))

else:
    st.info("Sol panelden seÃ§imleri yapÄ±p **'ğŸš€ Modeli EÄŸit ve Tahmin Yap'** butonuna basÄ±n.")

# -----------------------------------------------------------
# BitiÅŸ
# -----------------------------------------------------------
st.caption("Not: Bu sayfa demo amaÃ§lÄ±dÄ±r. Ã–zellik mÃ¼hendisliÄŸi, sÄ±nÄ±f dengesi ve zaman baÄŸÄ±mlÄ±lÄ±ÄŸÄ± Ã¼retim seviyesinde optimize edilmelidir.")
