# pages/3_ğŸ§­_SuÃ§_Tahmini.py
from __future__ import annotations
import sys, pathlib

# --- bootstrap ---
HERE = pathlib.Path(__file__).resolve()
ROOT = HERE.parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# --- gÃ¼venli constants import ---
try:
    from utils.constants import SF_TZ_OFFSET, KEY_COL, MODEL_VERSION, MODEL_LAST_TRAIN, CATEGORIES
except Exception:
    SF_TZ_OFFSET  = -7
    KEY_COL       = "geoid"
    MODEL_VERSION = "v0"
    MODEL_LAST_TRAIN = "-"
    CATEGORIES    = ["Assault","Burglary","Robbery","Theft","Vandalism","Vehicle Theft"]

import os, io, zipfile, requests
from datetime import datetime
import pandas as pd
import numpy as np
import streamlit as st
import pydeck as pdk
from components.last_update import show_last_update_badge


st.set_page_config(page_title="ğŸ”® SuÃ§ Tahmini (Stacking Model)", layout="wide")
st.title("ğŸ”® SuÃ§ Tahmini ve Model PerformansÄ±")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“¦ GitHub artifact'tan veri yÃ¼kleme
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# --- ayarlar ---
OWNER = "cem5113"
REPO  = "crime_prediction_data"
TARGET_ARTIFACT = "fr-crime-outputs-parquet"

def resolve_github_token() -> str | None:
    import os
    import streamlit as st
    for k in ("GITHUB_TOKEN","GH_TOKEN","github_token"):
        v = os.getenv(k) or (getattr(st, "secrets", {}).get(k) if hasattr(st, "secrets") else None)
        if v:
            os.environ["GITHUB_TOKEN"] = v
            return v
    return None

@st.cache_data(show_spinner=True)
def load_artifact_data() -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    1) Ã–nce artifact/risk_hourly.parquet (kÃ¼Ã§Ã¼k ve hazÄ±r) -> df_risk
    2) Yoksa fr_crime_09.parquet'ten sadece gerekli kolonlarÄ± okuyup GEOID bazÄ±nda risk hesapla
    3) Metrikleri artifact/metrics_stacking_ohe.parquet'ten yÃ¼kle (varsa)
    """
    token = resolve_github_token()
    if not token:
        st.error("GitHub Token bulunamadÄ± (GH_TOKEN / GITHUB_TOKEN).")
        return pd.DataFrame(), pd.DataFrame()

    headers = {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

    try:
        # 1) Son baÅŸarÄ±lÄ± run â†’ artifact listesi
        runs = requests.get(
            f"https://api.github.com/repos/{OWNER}/{REPO}/actions/runs?per_page=10",
            headers=headers, timeout=30
        ).json()
        run_ids = [r["id"] for r in runs.get("workflow_runs", []) if r.get("conclusion") == "success"]
        for rid in run_ids:
            arts = requests.get(
                f"https://api.github.com/repos/{OWNER}/{REPO}/actions/runs/{rid}/artifacts",
                headers=headers, timeout=30
            ).json().get("artifacts", [])

            target = next((a for a in arts if a["name"] == TARGET_ARTIFACT and not a.get("expired")), None)
            if not target:
                continue

            zdata = requests.get(target["archive_download_url"], headers=headers, timeout=60).content
            outer = zipfile.ZipFile(io.BytesIO(zdata))

            # iÃ§ zipâ€™i bul ve aÃ§
            inner_zip_name = next((n for n in outer.namelist() if n.lower().endswith(".zip")), None)
            if not inner_zip_name:
                st.error("Ä°Ã§ zip (Ã¶r. fr_parquet_outputs.zip) bulunamadÄ±.")
                return pd.DataFrame(), pd.DataFrame()
            nested = zipfile.ZipFile(io.BytesIO(outer.read(inner_zip_name)))

            names = nested.namelist()

            # ---- A) Ã–ncelik: kÃ¼Ã§Ã¼k dosya (hazÄ±r risk) ----
            risk_ready = next((n for n in names if n.endswith("artifact/risk_hourly.parquet")), None)

            # ---- B) Metrikler ----
            metrics_name = next((n for n in names if n.endswith("artifact/metrics_stacking_ohe.parquet")), None)
            metrics = pd.DataFrame()
            if metrics_name:
                try:
                    metrics = pd.read_parquet(io.BytesIO(nested.read(metrics_name)))
                except Exception as e:
                    st.warning(f"metrics_stacking_ohe.parquet okunamadÄ±: {e}")

            # ---- A yolu: risk_hourly varsa direkt onu kullan ----
            if risk_ready:
                df = pd.read_parquet(io.BytesIO(nested.read(risk_ready)))
                # Beklenen kolonlar: geoid, hour, risk (ya da score). Esnek adlandÄ±rma:
                low = {c.lower(): c for c in df.columns}
                gcol = low.get("geoid") or low.get("geoid_x") or low.get("id") or list(df.columns)[0]
                # hour kolonunu ÅŸart koÅŸma; sadece GEOID bazÄ±nda ortalama risk gÃ¶ster
                scol = low.get("risk") or low.get("score") or low.get("prob") or list(df.columns)[-1]
                df = (df[[gcol, scol]]
                      .rename(columns={gcol: "geoid", scol: "risk_score"})
                      .groupby("geoid", as_index=False)["risk_score"].mean())
                st.success("âœ… risk_hourly.parquet yÃ¼klendi (hafif).")
                return df, metrics

            # ---- B yolu: bÃ¼yÃ¼k dosyadan minimal okuma ----
            crime_name = next((n for n in names if n.endswith("fr_crime_09.parquet")), None)
            if not crime_name:
                st.error("fr_crime_09.parquet bulunamadÄ±.")
                return pd.DataFrame(), metrics

            # Sadece gerekli kolonlar
            candidate_cols = [
                "geoid","GEOID","id",
                "latitude","lat",
                "longitude","lon",
                "category",
                "event_hour","event_hour_x",
                "Y_label"
            ]
            # Parquet'te kolonlarÄ± gÃ¶rmeden â€˜columns=â€™ veremeyiz; Ã¶nce schema alalÄ±m
            tmp = pd.read_parquet(io.BytesIO(nested.read(crime_name)), columns=None)
            cols_exist = [c for c in candidate_cols if c in tmp.columns]

            # hafÄ±zayÄ± koru: yalnÄ±z bu kolonlarÄ± oku
            df_big = tmp[cols_exist].copy()
            del tmp  # memory

            # kolon adlarÄ±nÄ± normalize et
            low = {c.lower(): c for c in df_big.columns}
            gcol = low.get("geoid") or low.get("id")
            latc = low.get("latitude") or low.get("lat")
            lonc = low.get("longitude") or low.get("lon")
            ycol = low.get("y_label")
            hcol = low.get("event_hour") or low.get("event_hour_x")
            ccol = low.get("category")

            # Ã§ok bÃ¼yÃ¼kse Ã¶rnekle (Ã¶rn. 300k satÄ±r)
            N_MAX = 300_000
            if len(df_big) > N_MAX:
                df_big = df_big.sample(N_MAX, random_state=7)

            # risk: GEOID bazÄ±nda ortalama Y_label (yoksa olay sayÄ±sÄ±)
            if ycol and ycol in df_big.columns:
                risk = df_big.groupby(gcol)[ycol].mean().reset_index(name="risk_score")
            else:
                risk = df_big.groupby(gcol).size().reset_index(name="risk_score")

            # merkezler: lat/lon ortalamasÄ±
            if latc in df_big.columns and lonc in df_big.columns:
                centers = (df_big.groupby(gcol)[[latc, lonc]].mean()
                           .reset_index().rename(columns={gcol: "geoid", latc: "latitude", lonc: "longitude"}))
                risk = risk.rename(columns={gcol: "geoid"}).merge(centers, on="geoid", how="left")
            else:
                risk = risk.rename(columns={gcol: "geoid"})

            st.success("âœ… fr_crime_09.parquet minimal kolonlarla yÃ¼klendi.")
            return risk, metrics

        st.error(f"Artifact bulunamadÄ± ({TARGET_ARTIFACT}).")
        return pd.DataFrame(), pd.DataFrame()

    except Exception as e:
        st.error(f"Artifact indirilemedi/aÃ§Ä±lÄ±rken hata: {e}")
        return pd.DataFrame(), pd.DataFrame()

df, metrics = load_artifact_data()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ•’ GÃ¼ncel durum ve baÅŸlÄ±k
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
show_last_update_badge(
    data_upto=datetime.utcnow().strftime("%Y-%m-%d %H:%M"),
    model_version=MODEL_VERSION,
    last_train=MODEL_LAST_TRAIN,
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š Model Metrikleri
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ“ˆ Model Performans Ã–zeti")

if not metrics.empty:
    st.dataframe(metrics, use_container_width=True)
else:
    st.info("Model metrikleri bulunamadÄ± (metrics_stacking_ohe.parquet).")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”® Tahmin HaritasÄ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ“ SuÃ§ OlasÄ±lÄ±ÄŸÄ± HaritasÄ±")

if not df.empty:
    df = df.rename(columns=lambda c: c.strip())
    # GEOID ve koordinatlarÄ± kontrol et
    lat_col = next((c for c in df.columns if "lat" in c.lower()), "latitude")
    lon_col = next((c for c in df.columns if "lon" in c.lower()), "longitude")
    geoid_col = next((c for c in df.columns if "geoid" in c.lower()), "GEOID")

    # Filtreler
    st.sidebar.markdown("### ğŸ” Filtreler")
    cats = sorted(df["category"].dropna().unique().tolist()) if "category" in df.columns else []
    sel_cat = st.sidebar.selectbox("SuÃ§ Kategorisi", ["(TÃ¼mÃ¼)"] + cats)
    hour_range = st.sidebar.slider("Saat aralÄ±ÄŸÄ±", 0, 24, (0, 24))

    # Zaman filtresi
    if "event_hour_x" in df.columns:
        df = df[df["event_hour_x"].between(hour_range[0], hour_range[1])]
    if sel_cat != "(TÃ¼mÃ¼)" and "category" in df.columns:
        df = df[df["category"] == sel_cat]

    # Risk skorlarÄ±nÄ± oluÅŸtur (Ã¶rnek)
    if "Y_label" in df.columns:
        risk = df.groupby(geoid_col)["Y_label"].mean().reset_index(name="risk_score")
    else:
        risk = df.groupby(geoid_col).size().reset_index(name="risk_score")

    # CoÄŸrafi merkez hesaplama
    if lat_col in df.columns and lon_col in df.columns:
        geo = df.groupby(geoid_col)[[lat_col, lon_col]].mean().reset_index()
        risk = risk.merge(geo, on=geoid_col, how="left")

    # PyDeck haritasÄ±
    layer = pdk.Layer(
        "HeatmapLayer",
        data=risk,
        get_position=[lon_col, lat_col],
        get_weight="risk_score",
        radiusPixels=40,
        opacity=0.6,
    )

    view = pdk.ViewState(
        latitude=risk[lat_col].mean(),
        longitude=risk[lon_col].mean(),
        zoom=11.2,
        pitch=30,
    )

    st.pydeck_chart(pdk.Deck(map_style="mapbox://styles/mapbox/light-v11", initial_view_state=view, layers=[layer]))

    # En riskli GEOID'ler
    st.subheader("ğŸš¨ En Riskli 10 GEOID")
    top10 = risk.sort_values("risk_score", ascending=False).head(10)
    st.dataframe(top10, use_container_width=True)

    st.caption("Not: Risk skoru, GEOID bazÄ±nda suÃ§ gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ±n normalize edilmiÅŸ deÄŸeridir.")
else:
    st.warning("Veri yÃ¼klenemedi veya boÅŸ. Artifact baÄŸlantÄ±sÄ±nÄ± kontrol edin.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§  SonuÃ§ Ã–zeti
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("---")
st.markdown(
    """
    **ğŸ§  Ã–zet:**  
    Bu sayfa, stacking tabanlÄ± modelin son tahmin Ã§Ä±ktÄ±sÄ±nÄ± `fr_crime_09.csv` Ã¼zerinden yÃ¼kler ve  
    model metriklerini `metrics_stacking_ohe.parquet` dosyasÄ±ndan alÄ±r.  
    Harita, her GEOID iÃ§in normalize edilmiÅŸ suÃ§ riski yoÄŸunluÄŸunu gÃ¶sterir.  
    """
)
